{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3436994",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Floating Point Numbers\n",
    "\n",
    "## What is a Floating Point Number?\n",
    "\n",
    "**Definition**: A method to represent real numbers (with decimals) in binary using scientific notation.\n",
    "\n",
    "**Why \"Floating Point\"?** The decimal point can \"float\" to accommodate a wide range of values.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **Sign bit**: Indicates positive (0) or negative (1)\n",
    "- **Exponent**: Determines magnitude (with bias)\n",
    "- **Mantissa / Significand**: Encodes the significant digits (fractional part)\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\\text{Value} = (-1)^{\\text{sign}} \\times \\text{mantissa} \\times 2^{\\text{exponent}}$$\n",
    "\n",
    "## IEEE 754 Standard Format\n",
    "\n",
    "### 32-bit Float (Single Precision)\n",
    "\n",
    "```\n",
    "| S | Exponent (8 bits) | Mantissa (23 bits) |\n",
    "| 1 | 8 bits           | 23 bits            |\n",
    "```\n",
    "\n",
    "### 64-bit Double (Double Precision)\n",
    "\n",
    "```\n",
    "| S | Exponent (11 bits) | Mantissa (52 bits) |\n",
    "| 1 | 11 bits            | 52 bits            |\n",
    "```\n",
    "\n",
    "### Precision & Range\n",
    "\n",
    "| Format        | Precision             | Range               |\n",
    "|---------------|-----------------------|---------------------|\n",
    "| 32-bit float  | ~7 decimal digits     | ~10⁻³⁸ to ~10³⁸    |\n",
    "| 64-bit double | ~15–16 decimal digits | ~10⁻³⁰⁸ to ~10³⁰⁸  |\n",
    "\n",
    "## Example: Converting 12.375\n",
    "\n",
    "### Step-by-Step Conversion\n",
    "\n",
    "**Step 1: Convert to Binary**\n",
    "- 12.375₁₀ = 1100.011₂ (12 = 1100, 0.375 = 0.011)\n",
    "\n",
    "**Step 2: Normalize**\n",
    "- 1100.011₂ = 1.100011 × 2³\n",
    "\n",
    "**Step 3: Extract Components**\n",
    "- **Sign (S)** = 0\n",
    "- **Exponent (E)** = 3 + 127 (bias) = 130 = `10000010`\n",
    "- **Mantissa (M)** = `10001100000000000000000` (excluding implicit `1`)\n",
    "\n",
    "```\n",
    "Bit Pattern: 0 | 10000010 | 10001100000000000000000\n",
    "Hexadecimal: 0x41460000\n",
    "```\n",
    "\n",
    "## Why is Bias Added to Exponent?\n",
    "\n",
    "### The Problem Without Bias\n",
    "\n",
    "- Signed integers complicate comparison\n",
    "- Encoding special values becomes harder\n",
    "\n",
    "### IEEE 754 Bias System\n",
    "\n",
    "- **Single Precision (32-bit)**: Bias = 127\n",
    "- **Formula**: $\\text{Stored Exponent} = \\text{Actual Exponent} + \\text{Bias}$\n",
    "\n",
    "### Benefits of Bias\n",
    "\n",
    "1. **Simplified Comparisons** → Larger exponent = larger number\n",
    "2. **Natural Ordering** → Binary comparison works directly\n",
    "3. **Encodes Special Values** → Reserved exponent values for special cases\n",
    "\n",
    "## BF16 (Brain Floating Point) Format\n",
    "\n",
    "### BF16 Structure\n",
    "\n",
    "```\n",
    "| S | Exponent (8 bits) | Mantissa (7 bits) |\n",
    "| 1 | 8 bits           | 7 bits           |\n",
    "```\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- Same exponent range as FP32 (bias = 127)\n",
    "- Only 7 bits for mantissa → ~2–3 digits precision\n",
    "- Truncated FP32: Easy hardware conversion\n",
    "\n",
    "### Simple Conversion\n",
    "\n",
    "```c\n",
    "bf16 = fp32 >> 16;\n",
    "```\n",
    "\n",
    "## Format Comparison (Float32, BF16, and Float16)\n",
    "\n",
    "| Format | Sign | Exponent | Mantissa | Range           | Precision    |\n",
    "|--------|------|----------|----------|-----------------|--------------|\n",
    "| FP32   | 1    | 8        | 23       | ~10⁻³⁸–10³⁸    | ~7 digits    |\n",
    "| BF16   | 1    | 8        | 7        | ~10⁻³⁸–10³⁸    | ~2–3 digits  |\n",
    "| FP16   | 1    | 5        | 10       | ~10⁻⁸–10⁴      | ~3–4 digits  |\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "| Format | Use Case                           |\n",
    "|--------|------------------------------------|\n",
    "| FP32   | Scientific, GPU computing          |\n",
    "| FP64   | High-precision scientific tasks    |\n",
    "| BF16   | Deep learning (training/inference) |\n",
    "| FP16   | Mobile, edge, low-power devices    |\n",
    "\n",
    "## Why BF16 in AI/ML?\n",
    "\n",
    "### Four Key Advantages\n",
    "\n",
    "1. **Wide Range**: Can handle large/small weights\n",
    "   - Same exponent range as FP32\n",
    "\n",
    "2. **Efficient Conversion**:\n",
    "   - Simple bit shifting operation\n",
    "   - No complex rounding needed\n",
    "\n",
    "3. **Stable Gradients**:\n",
    "   - Prevents overflow/underflow during training\n",
    "\n",
    "4. **Memory Efficient**:\n",
    "   - Half the size of FP32\n",
    "   - Faster data transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf04e75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
